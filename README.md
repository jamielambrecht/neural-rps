# RPS Image Detection Game

## Introduction
This program is a school project for Artificial Intelligence class. It allows the user to add a layer to a NasNET Mobile Convolutional Neural Network model with their own images of their hand doing gestures for "rock", "paper", "scissors", and "nothing", which can then be tested and used in a simple "RPS" game.

## Dependencies
Python 3.9.9

### Pip packages
opencv-python
tensorflow
numpy
sklearn
scipy

## Installation
Install the correct version of Python and ensure that you can run Python files (may require configuring the OS or shell environment variables, i.e. adding python to your "PATH".). Install the Python package installer "pip" using the command "python -m ensurepip --upgrade". The name of the python command may be "python3" on some Linux distributions, etc. Use pip to install the dependencies. This can be done using the provided file "requirements.txt" with the following command: "python -m pip install -r requirements.txt" or just "pip install -r requirements.txt" (If pip is also in your PATH). This program was tested on Windows 11 and Fedora Linux 35 and was found to need no further configuration. We can not guarantee compatability of the libraries with any particular hardware/operating system configurations.

## Usage
Run the program using "python main.py". The application UI is fragmented so interaction with the program occurs in both the terminal as well as in windows that will appear at different stages of the program. Warnings regarding Nvidia GPU drivers are likely and can be ignored.

When the program begins, the user will be prompted at the terminal whether they would like to load a previously saved model. Initially these files will not exist, so regardless of the answer, the user will be shown a screen for gathering training data images. 

After going through the training process once, the user will have the option to load the previous model instead of going through this relatively time-consuming process. When the training screen appears, the user will see the default video camera feed, assuming the operating system has configured this in a way that OpenCV detects by default. We do not currently have options to set custom input devices, etc. 

During the training process, the user should hold up their hand into the box in the camera feed and make a "rock", "paper", or "scissors" hand gesture and press "r", "p", or "s", respectively. When the key is pressed, the program will begin capturing a set of 100 images for each gesture type. The user should move their hand slightly during the image capture process to introduce variation into the dataset, which will improve the recognition ability. The user must also press "n" and leave their hand out of the box in order to create a set of images labeled as "nothing", as a control group. After gathering all of the image sets, press "q" to close the window and move on to the next stage.

At this stage, the console will display output from the TensorFlow training process as each "epoch" of the training process is executed. This can take several minutes and is likely to use upwards of 8GB of RAM. There is currently no mechanism to protect the memory usage and it is very possible to experience crashes due to running out of memory and/or swap capacity.

After the training process is complete, the user will be prompted in the console to choose whether they would like to test the model for accuracy. If the user response is "y" for "yes", a window will appear with the camera feed and the box again, but this time, the window will display text showing the prediction generated by the model for the current frame along with a confidence number as a percentage. The default confidence threshold used in the game playing functionality is 70%, so it is ideal that a number above this threshold is shown with reasonable consistency along with the correct category label whenever the user makes a gesture in the box. Press 'q' to exit the testing screen.

After this, another window will appear with the same camera feed and image box, but with UI text showing a score for the Player and the Computer. Whenever the user places a hand gesture into the box, the model will determine the classification of the gesture and compare it to a randomly chosen move on behalf of the Computer agent. Each time a new gesture is detected, the box will turn green if the Player's move beats the computer, red if Computer beats the Player, and white if there is a tie. The winner is determined by the greater score accumulated in 5 matches by default. After 5 matches, the winner of the game is displayed. At this point, the user can press Enter to play again, or any other key to exit the program. 


